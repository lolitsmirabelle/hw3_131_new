---
title: "Homework 3"
output: html_document
date: '2022-04-16'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidymodels)
library(ISLR) 
library(ISLR2) 
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) # for naive bayes
tidymodels_prefer()
```

## Question 1

```{r}
titanic <- read.csv("~/Downloads/homework-3/data/titanic.csv")
titanic$survived <- factor(titanic$survived,levels=c('Yes', 'No'))
titanic$pclass <- factor(titanic$pclass)

set.seed(2424)

titanic_split <- initial_split(titanic, prop = 0.80,
                                strata = survived)
titanic_train <- training(titanic_split) 
titanic_test <- testing(titanic_split)
```
It is a good idea to use stratified sampling for this data since each subgroup of the given population is adequately represented within the whole sample population.


## Question 2

```{r}
titanic_train %>% 
  ggplot(aes(x = survived)) +
  geom_bar()
```

We can see from the graph that there are much more people who did not survive versus those who did survive. 


## Question 3

```{r}
cor_titanic_train <- titanic_train %>%
  select(-c(survived, pclass, name, sex, ticket, cabin, embarked)) %>%
  correlate()
rplot(cor_titanic_train)
```

From the correlation matrix, we can see that sib_sp and age are negatively related, parch and age are negatively related, parch and sib_sp are positively related, fare and sib_sp are positively related, and fare and parch are positively related.


## Question 4

```{r}
titanic_recipe <- 
  recipe(survived ~ ., data = titanic_train%>%select(survived, sex, age, sib_sp, pclass, fare, parch)) %>% 
  step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ starts_with("sex"):fare + age:fare) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
```


## Question 5

```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)

log_fit <- fit(log_wkflow, titanic_train)
```

## Question 6

```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)

lda_fit <- fit(lda_wkflow, titanic_train)
```


## Question 7

```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)

qda_fit <- fit(qda_wkflow, titanic_train)
```


## Question 8

```{r}
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE) 

nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(titanic_recipe)

nb_fit <- fit(nb_wkflow, titanic_train)
```


## Question 9 
### Logistic Regression
```{r}
predict(log_fit, new_data = titanic_train, type = "prob")
augment(log_fit, new_data = titanic_train) %>%
  conf_mat(truth = survived, estimate = .pred_class)
log_reg_acc <- augment(log_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
log_reg_acc
```

### LDA 
```{r}
predict(lda_fit, new_data = titanic_train, type = "prob")
augment(lda_fit, new_data = titanic_train) %>%
  conf_mat(truth = survived, estimate = .pred_class) 
lda_acc <- augment(lda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
lda_acc
```

### QDA
```{r}
predict(qda_fit, new_data = titanic_train, type = "prob")
augment(qda_fit, new_data = titanic_train) %>%
  conf_mat(truth = survived, estimate = .pred_class) 
qda_acc <- augment(qda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
qda_acc
```

### Naive Bayes
```{r}
predict(nb_fit, new_data = titanic_train, type = "prob")
augment(nb_fit, new_data = titanic_train) %>%
  conf_mat(truth = survived, estimate = .pred_class) 
nb_acc <- augment(nb_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
nb_acc
```


```{r}
accuracies <- c(log_reg_acc$.estimate, lda_acc$.estimate, 
                nb_acc$.estimate, qda_acc$.estimate)
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")
results <- tibble(accuracies = accuracies, models = models)
results %>% 
  arrange(-accuracies)
```
Out of all of the models, the model that achieved the highest accuracy was the logistic regression model. 


## Question 10 

```{r}
log_reg_test <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow_test <- workflow() %>% 
  add_model(log_reg_test) %>% 
  add_recipe(titanic_recipe)

log_fit_test <- fit(log_wkflow_test, titanic_test)

predict(log_fit_test, new_data = titanic_test, type = "prob")
augment(log_fit_test, new_data = titanic_test) %>%
  conf_mat(truth = survived, estimate = .pred_class)
log_reg_acc_test <- augment(log_fit_test, new_data = titanic_test) %>%
  accuracy(truth = survived, estimate = .pred_class)
log_reg_acc_test
```
```{r}
augment(log_fit_test, new_data = titanic_test) %>%
  conf_mat(truth = survived, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

```{r}
augment(log_fit_test, new_data = titanic_test) %>%
  roc_curve(survived, .pred_Yes) %>%
  autoplot()
```

```{r}
augment(log_fit, new_data = titanic_test)%>%
  roc_auc(truth=survived,estimate=.pred_Yes)
```


The accuracy of the testing model was 0.8324022 which was higher than its training model which had an accuracy of 0.8047753.The values differ because training data is the initial dataset used to teach a machine learning application to recognize patterns, while testing data is used to evaluate the model's accuracy.
